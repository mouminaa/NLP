{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moumi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle entraîné et sauvegardé sous word2vec_russian.model\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "# Скачать русские стоп-слова\n",
    "# Téléchargez les stopwords russes\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Загрузите и прочитайте текстовый файл\n",
    "# Charger et lire le fichier texte\n",
    "file_path = \"news.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = f.readlines()\n",
    "\n",
    "# Инициализируем морфологический анализатор и стоп-слова\n",
    "# Initialiser l'analyseur morphologique et les stopwords\n",
    "morph = MorphAnalyzer()\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "# Функция предварительной обработки\n",
    "# Fonction de prétraitement\n",
    "def preprocess_text(text):\n",
    "    tokens = gensim.utils.simple_preprocess(text, deacc=True)  # Tokenisation de base\n",
    "    tokens = [morph.normal_forms(word)[0] for word in tokens if word not in stop_words]  # Lemmatisation et suppression des stopwords\n",
    "    return tokens\n",
    "\n",
    "# Предварительная обработка корпуса\n",
    "# Prétraitement du corpus\n",
    "processed_corpus = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Обучение модели Word2Vec\n",
    "# Entraîner le modèle Word2Vec\n",
    "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Сохранить модель\n",
    "# Sauvegarder le modèle\n",
    "model_path = \"word2vec_russian.model\"\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Modèle entraîné et sauvegardé sous {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement : (8000, 100)\n",
      "Taille de l'ensemble de test : (2000, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_path = \"word2vec_russian.model\"\n",
    "model = Word2Vec.load(model_path)\n",
    "\n",
    "# Функция для вычисления среднего значения векторов слов для документа\n",
    "# Fonction pour calculer la moyenne des vecteurs de mots pour un document\n",
    "def document_to_vector(document, model):\n",
    "    vectors = []\n",
    "    for word in document:\n",
    "        if word in model.wv:  # Vérifier si le mot est dans le vocabulaire\n",
    "            vectors.append(model.wv[word])\n",
    "    if vectors:  # Si des vecteurs sont trouvés\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:  # Si aucun vecteur n'est trouvé\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Разделить тексты на обучающий и тестовый наборы\n",
    "# Diviser les textes en ensembles d'entraînement et de test\n",
    "train_texts, test_texts = train_test_split(processed_corpus, test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование текстов в векторы\n",
    "# Mapper les textes en vecteurs\n",
    "train_vectors = np.array([document_to_vector(doc, model) for doc in train_texts])\n",
    "test_vectors = np.array([document_to_vector(doc, model) for doc in test_texts])\n",
    "\n",
    "# Проверка\n",
    "# Vérification\n",
    "print(\"Taille de l'ensemble d'entraînement :\", train_vectors.shape)\n",
    "print(\"Taille de l'ensemble de test :\", test_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.46      0.49      1016\n",
      "           1       0.50      0.56      0.53       984\n",
      "\n",
      "    accuracy                           0.51      2000\n",
      "   macro avg       0.51      0.51      0.51      2000\n",
      "weighted avg       0.51      0.51      0.51      2000\n",
      "\n",
      "Accuracy Score: 0.508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Пример надписей к текстам\n",
    "# текста связаны с категориями/классом (например, 0, 1 и т. д.)\n",
    "# Пример: train_labels = [0, 1, 1, 0, ...] и test_labels = [0, 1, ...]\n",
    "# Exemple d'étiquettes pour les textes \n",
    "# le textes sont associés à des catégories/classe (par ex., 0, 1, etc.)\n",
    "# Exemple : train_labels = [0, 1, 1, 0, ...] et test_labels = [0, 1, ...]\n",
    "train_labels = np.random.randint(0, 2, size=len(train_vectors))  # Remplacez par vos vraies étiquettes\n",
    "test_labels = np.random.randint(0, 2, size=len(test_vectors))    # Remplacez par vos vraies étiquettes\n",
    "\n",
    "# Обучение модели SVM\n",
    "# Entraîner un modèle SVM\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_vectors, train_labels)\n",
    "\n",
    "# Прогнозирование классов на тестовом наборе\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "predicted_labels = svm_model.predict(test_vectors)\n",
    "\n",
    "# Оцените производительность\n",
    "# Évaluer les performances\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, predicted_labels))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(test_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Создаем представление TF-IDF\n",
    "# Construire une représentation TF-IDF\n",
    "texts_joined = [\" \".join(doc) for doc in processed_corpus]  # Joindre les tokens pour chaque document\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts_joined)\n",
    "\n",
    "# Извлекаем слова и их веса TF-IDF\n",
    "# Récupérer les mots et leurs poids TF-IDF\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "word2tfidf = dict(zip(vocab, tfidf.idf_))\n",
    "\n",
    "# Функция для расчета взвешенных векторов по TF-IDF\n",
    "# Fonction pour calculer les vecteurs pondérés par TF-IDF\n",
    "def document_to_tfidf_vector(document, model, word2tfidf):\n",
    "    vectors = []\n",
    "    for word in document:\n",
    "        if word in model.wv and word in word2tfidf:  # Vérifier si le mot est dans le vocabulaire et dans TF-IDF\n",
    "            vectors.append(model.wv[word] * word2tfidf[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Рассчитать взвешенные векторы для всех документов\n",
    "# Calculer les vecteurs pondérés pour l'ensemble des documents\n",
    "train_vectors_tfidf = np.array([document_to_tfidf_vector(doc, model, word2tfidf) for doc in train_texts])\n",
    "test_vectors_tfidf = np.array([document_to_tfidf_vector(doc, model, word2tfidf) for doc in test_texts])\n",
    "\n",
    "# Нормализовать векторы\n",
    "# Normaliser les vecteurs\n",
    "train_vectors_tfidf = normalize(train_vectors_tfidf)\n",
    "test_vectors_tfidf = normalize(test_vectors_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Создание помеченных документов\n",
    "# Créer des documents étiquetés\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(processed_corpus)]\n",
    "\n",
    "# Обучение модели Doc2Vec\n",
    "# Entraîner le modèle Doc2Vec\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, epochs=20)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# Получить векторы документов\n",
    "# Obtenir les vecteurs des documents\n",
    "train_vectors_doc2vec = np.array([doc2vec_model.dv[str(i)] for i in range(len(train_texts))])\n",
    "test_vectors_doc2vec = np.array([doc2vec_model.dv[str(i)] for i in range(len(test_texts))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.28      0.36      1016\n",
      "           1       0.49      0.72      0.58       984\n",
      "\n",
      "    accuracy                           0.49      2000\n",
      "   macro avg       0.50      0.50      0.47      2000\n",
      "weighted avg       0.50      0.49      0.47      2000\n",
      "\n",
      "Accuracy Score: 0.4945\n"
     ]
    }
   ],
   "source": [
    "# Реформируйте алгоритм SVM с помощью взвешенных векторов TF-IDF или Doc2Vec.\n",
    "# Reformez l'algorithme SVM avec les vecteurs TF-IDF pondérés ou Doc2Vec\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(train_vectors_tfidf, train_labels)  # ou train_vectors_doc2vec\n",
    "\n",
    "# Прогнозирование классов на тестовом наборе\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "predicted_labels = svm_model.predict(test_vectors_tfidf)  # ou test_vectors_doc2vec\n",
    "\n",
    "# Оцените производительность\n",
    "# Évaluer les performances\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, predicted_labels))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(test_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
